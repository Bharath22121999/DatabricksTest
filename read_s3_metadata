if 'Contents' in objects:
    # Initialize lists to store metadata information
    source_paths = []
    filenames = []
    last_modified_dates = []
    sizes = []

    for obj in objects.get('Contents', []):
        # Extract metadata information
        source_path = obj['Key']
        filename = source_path.rsplit('/', 1)[-1]
        last_modified = obj['LastModified']
        size = obj['Size']

        # Convert datetime to string
        last_modified_str = last_modified.strftime("%Y-%m-%d %H:%M:%S")

        # Append metadata information to lists
        source_paths.append(source_path)
        filenames.append(filename)
        last_modified_dates.append(last_modified_str)
        sizes.append(size)

    # Create a Pandas DataFrame with the metadata
    metadata_df = pd.DataFrame({
        'SourcePath': source_paths,
        'Filename': filenames,
        'LastModified': last_modified_dates,
        'Size': sizes
    })

    # Define the schema for the PySpark DataFrame
    schema = StructType([
        StructField("SourcePath", StringType(), True),
        StructField("Filename", StringType(), True),
        StructField("LastModified", TimestampType(), True),
        StructField("Size", StringType(), True)  # Change StringType to the appropriate type if 'Size' is not a string
    ])

    # Convert Pandas DataFrame to PySpark DataFrame
    spark_df = spark.createDataFrame(metadata_df, schema=schema)

    # Convert the timestamp column from string to PySpark timestamp
    spark_df = spark_df.withColumn("LastModified", from_unixtime(unix_timestamp("LastModified", "yyyy-MM-dd HH:mm:ss")))

    # Display the PySpark DataFrame
    spark_df.show()
else:
    print("No contents found in the specified S3 bucket and prefix.")
